---
title: "p8105_hw5_jl5934"
author: "Jesus Luevano"
date: "2023-11-03"
output: github_document
---

```{r}
library(tidyverse)
```

# Problem 1 - WaPo

```{r WaPo dataset read}
WaPo.df <- read_csv("data/homicide-data.csv") %>%
  janitor::clean_names()
```

The data includes `r print(colnames(WaPo.df))`, with a total of `r nrow(WaPo.df)` observations. 

```{r WaPo tidy}
unsolved <- WaPo.df %>%
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>%
  mutate(city_state = 
           paste0(city,", ",state)) %>%
  group_by(city_state) %>%
  summarize(unsolved = n()) 

total_homicides <- WaPo.df %>%
  mutate(city_state = 
           paste0(city,", ",state)) %>%
  group_by(city_state) %>%
  summarize(total = n()) 

solved <- WaPo.df %>%
  filter(disposition %in% c("Closed by arrest")) %>%
  mutate(city_state = 
           paste0(city,", ",state)) %>%
  group_by(city_state) %>%
  summarize(solved = n())

```

```{r WaPO filter,eval=FALSE }
prop_homicide <- full_join(unsolved, solved, by = "city_state") %>%
  replace(is.na(.), 0) %>%
  filter(city_state == "Baltimore, MD") %>%
  column_to_rownames(var = "city_state") %>%
    as.matrix() 

test <- broom::tidy(prop.test(prop_homicide))

tibble(
  prop_unsolved = test[["estimate"]],
  CI_low = test[["conf.low"]],
  CI_upper = test[["conf.high"]]
)

```

```{r}
cities = as.vector(total_homicides[["city_state"]])

prop_hom = function(x) {
  
  prop_homicide <- full_join(unsolved, solved, by = "city_state") %>%
  replace(is.na(.), 0) %>%
  filter(city_state == x) %>%
  column_to_rownames(var = "city_state") %>%
    as.matrix() 

test <- broom::tidy(prop.test(prop_homicide))

tibble(
  city = x,
  prop_unsolved = test[["estimate"]],
  CI_low = test[["conf.low"]],
  CI_upper = test[["conf.high"]]
)
  
}

WaPo_output = map(cities, prop_hom) %>%
  bind_rows()
```

```{r}
ggplot(WaPo_output, aes(x = fct_reorder(city, prop_unsolved), y = prop_unsolved)) +
  geom_point(stat = "identity") +
  geom_errorbar(aes(ymin = CI_low, ymax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 45, size = 5, hjust = 1))
```

* We see that there is some variability amongst the rates of unsolved murders, with the lowest at the outler of Chicago (showing a very high proportion overall of not being solved). 

* There is one other outlier with Tulsa, AL where the one murder was indeed solved. 


# Problem 2

First we will read in the data, unnest it as we read in multiple sheets, and then clean it up to a tidy dataset with variables we require for further analysis. 

```{r prep trial data}
file_list <- list.files("data/data/")

file_reader = function(x) {
  
  data_x = read_csv(paste0("data/data/", x))
  name_x = gsub(".csv","",x)
  
  tibble(
    name = name_x,
    data = as.data.frame(data_x)
  ) %>% 
  unnest(cols = data)
    
}

output = map(file_list, file_reader) %>%
  bind_rows() %>%
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "value"
  ) %>%
  rename("subject" = "name") %>%
  mutate(group = 
           ifelse(subject %in% c("con_01","con_02","con_03","con_04","con_05","con_06","con_07","con_08","con_09","con_10"), "control", "experimental"))

```

We will then create a plot showing the change by week of each subject, split by which arm of the study they were. 

```{r plot trial data}
ggplot(data = output, aes(x = week, y = value, group = subject)) +
  geom_point(aes(color = subject)) +
  geom_line(aes(color = subject)) +
  facet_grid(~group)

```

* Looking at the plot, we can see that in general the control group had lower values than the experimental, with some actually going negative, but the experimental group had much more change over time. 

* Specifically, they started at similar values centered around 2.5, but their mean at the end of the 8-week study was closer to 1.25, whereas the experimental group increased by the end of the study to general mean closer to 5 among all the subjects. This shows within subjects in a group there is significant association as to where their final outcome value will be based on their starting. 



# Problem 3

We will first create a simulation function based on parameters set at n=30, sigma=5, but mu that varies form 0-6. We will need to save this into a dataframe within a list. 

We will use the t.test function for a one-sample t-test to evaluate if our mean mu-hat was significantly different from an expected population mean of 0.

Lastly, we will mutate a variable that relates to if we rejected the null hypothesis, based on whether the p-value from the t-test was <0.05. 

```{r}
sim_mean_test = function(n = 30, mu, sigma = 5) {
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
    )
  
  output = sim_data %>% 
    summarize(
      mu_hat = mean(x),
      p_value = broom::tidy(t.test(x)) %>% pull("p.value")
    )
  
  return(output)
} 

mu_list = c(0:6)

output_sim = expand_grid(
    iterations = 1:5000, 
    mu = mu_list
  ) %>%
  mutate(
    output_sim_df = pmap(list(n = 30, mu = mu, sigma = 5), sim_mean_test)
  ) %>%
  unnest(output_sim_df)

output_sim <- output_sim %>%
  mutate(significant = if_else(p_value < 0.05, "reject_null", "not_reject_null"))
```

From our dataset created above, we will create a plot looking at the proportion of null hypotheses rejected (i.e. p-value <0.05) grouped by mean value 0:6. This will allow us to see trends of power to find a true difference in mean, based on sample mean different from population (a surrogate for the effect size).

```{r plot power}
ggplot(output_sim, aes(x = mu, fill = factor(significant))) +
  geom_bar(position = "fill") +
  labs(title = "Proportion of rejection by change in sample mean, Power analysis by effect Size", fill = "Null Rejection") + 
  xlab("True mean(Mu)") + ylab("Proportion")
       
```

* Looking at the power among different population means, the proportion where null rejected is about 0.05 was when sample mean was 0 and population mean was 0. 

* However, as effect size (mean increases from 0 to 1:6) the power should increase, i.e. we should note that the number of rejections increases as it is easier to distinguish a difference from 0 to 1:6 with stable SD = 5, which is the case. By the time we reach 4-6, nearly the entirety of nulls are rejected. 


Next we will create plots to assess if our average mean (mu_hat) for our simulation varies across set mu values for those that meet criteria for null hypothesis rejection, vs all simulations. 

```{r plot mean sample vs population}
output_sim %>%
  group_by(mu) %>%
  summarize(avg_mu_hat = mean(mu_hat)) %>%
  ggplot(aes(x = mu, y = avg_mu_hat)) + 
  geom_point() +
  geom_line() +
  labs(title = "Mean set compared to mu_hat average of all tests") +
    xlab("True Mu value") + ylab("Average mu_hat")

output_sim %>%
  filter(significant == "reject_null") %>%
  group_by(mu) %>%
  summarize(avg_mu_hat = mean(mu_hat)) %>%
  ggplot(aes(x = mu, y = avg_mu_hat)) + 
  geom_point() +
  geom_line() +
  labs(title = "Mean set compared to mu_hat average of tests where null hypothesis was rejected") +
    xlab("True Mu value") + ylab("Average mu_hat")


```

* We do see that the average value of mu-hat for tests where we include all the samples is nearly a straight line meaning mu ~ mu_hat. 

* However, when we look at the lines for the points where we only include data from where the null hypothesis was rejected, it is less straight, in particular at the lower values of mu (improving at >4). This is likely because as the effect size increases with our mu further from 0, it is more likely to reject the null that are more extreme, leaving us with bigger range of mu-hat. However, when the effect size is larger with larger mu, it is easier to reject those that where mu ~ mu_hat, and gives the straighter line that we see. 
